---
# Note: Spark versions starting with 2.0 are built with Scala 2.11 by
# default
spark_version: "1.6.2"
# Other choices might look like without-hadoop, hadoop1, hadoop2.3,
# hadoop2.4, hadoop2.6, hadoop2.7 -- look up valid combinations at
# http://archive.apache.org/dist/spark/
spark_hadoop_version: "hadoop1-scala2.11"
spark_is_master: True
# domain name of spark master
spark_master: "localhost"
# port of spark master for slaves to connect to
spark_master_port: 7077
# List of domain names for slaves
spark_slaves: []
spark_user: "{% if is_integration_test is defined and is_integration_test %}kitchen{% else %}spark{% endif %}"
spark_user_shell: "/bin/bash"
spark_tgz_filename: spark-{{spark_version}}-bin-{{spark_hadoop_version}}.tgz
# Set this directory to download and cache tarball onto local machine,
# for when the network connection between local and host is faster
# than between host and internet, and if role will be run for several
# hosts.
# spark_local_download_cache: .
spark_tgz_url_base: http://d3kbcqa49mib13.cloudfront.net/
spark_tgz_url: "{{spark_tgz_url_base}}{{spark_tgz_filename}}"
spark_install_chdir: /opt
spark_install_dir: "{{spark_install_chdir}}/spark"
spark_start_script: "{{spark_install_dir}}{% if spark_is_master
%}/sbin/start-master.sh{% else %}/sbin/start-slave.sh
spark://{{spark_master}}:{{spark_master_port}}{% endif %}"
spark_stop_script: "{{spark_install_dir}}{% if spark_is_master
%}/sbin/stop-master.sh{% else %}/sbin/stop-slave.sh{% endif %}"
spark_tmp_unpack_dir: "/tmp/spark-{{spark_version}}-{{spark_hadoop_version}}"
spark_log_dir: /var/log/spark
spark_pid_dir: /var/run/spark
spark_pidfile: "{{spark_pid_dir}}/spark--{% if
spark_is_master %}org.apache.spark.deploy.master.Master{% else
%}org.apache.spark.deploy.worker.Worker{% endif %}-1.pid"
# Configuration for spark-env.sh
spark_env:
  SPARK_LOG_DIR: "{{spark_log_dir}}"
  SPARK_PID_DIR: "{{spark_pid_dir}}"
  SPARK_WORKER_CORES: 6
# Site specific spark-env.sh settings
spark_env_site: {}
